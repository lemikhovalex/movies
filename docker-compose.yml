version: '3.9'

x-admin-db-common:
  &admin-db-common
  environment:
    &admin-db-common-env
    DB_HOST: ${DB_HOST}
    DB_PORT: ${DB_PORT}
    DB_PASSWORD: ${DB_PASSWORD}
    DB_USER: ${DB_USER}
    DB_NAME: ${DB_NAME}
    DJANGO_SUPERUSER_PASSWORD: ${DJANGO_SUPERUSER_PASSWORD}
    DJANGO_SUPERUSER_USERNAME: ${DJANGO_SUPERUSER_USERNAME}
    DJANGO_SUPERUSER_EMAIL: ${DJANGO_SUPERUSER_EMAIL}

x-es-common:
  &es-common
  environment:
    &es-common-env
    ES_HOST: ${ES_HOST}
    ES_PORT: ${ES_PORT}

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: ./etc/compose/airflow/Dockerfile
    args:
      DEBUG: ${DEBUG}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_AF_USER}:${DB_AF_PASSWORD}@af_db:${DB_AF_PORT}/${DB_AF_NAME}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${DB_AF_USER}:${DB_AF_PASSWORD}@af_db:${DB_AF_PORT}/${DB_AF_NAME}
    AIRFLOW__CELERY__BROKER_URL: redis://:@${REDIS_HOST}:${REDIS_PORT}/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__WEBSERVER__BASE_URL: 'http://localhost:${AF_PORT}/airflow'
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
    AF_PORT: ${AF_PORT}
    SPARK_MASTER_HOST: ${SPARK_MASTER_HOST}
    SPARK_MASTER_PORT: ${SPARK_MASTER_PORT}
    REDIS_PORT: ${REDIS_PORT}
    <<: *admin-db-common-env
    <<: *es-common-env
  volumes:
    - ./src/airflow/dags:/opt/airflow/dags
    - af_logs:/opt/airflow/logs
    - af_plugins:/opt/airflow/plugins
    - ./tests:/srv/app/tests
    - ./src/etl:/srv/app/src/etl
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-0}"
  depends_on:
    af_db:
      condition: service_healthy

services:
  admin_db:
    build:
      context: ./etc/compose/admin_db
    restart: always
    environment:
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USER}
      POSTGRES_DB: ${DB_NAME}
      PGDATA: "/var/lib/postgresql/data"
    volumes:
      - admin_db_vol:/var/lib/postgresql/data
    command: -p ${DB_PORT}

  elasticsearch:
    image: elasticsearch:8.6.2
    environment:
      http.port: ${ES_PORT}-9300
      cluster.routing.allocation.disk.threshold_enabled: false
      xpack.security.enabled: false
      discovery.type: single-node
      ES_JAVA_OPTS: -Xms750m -Xmx750m

  admin_panel:
    container_name: admin
    build:
      dockerfile: ./etc/compose/admin_panel/Dockerfile
      context: .
    volumes:
      - admin_static:/app/app_data/static
    environment:
      <<: *admin-db-common-env
      <<: *es-common-env
      ADMIN_PORT: ${ADMIN_PORT}
      SECRET_KEY: ${SECRET_KEY}
      WAIT_HOSTS: ${DB_HOST}:${DB_PORT}, ${ES_HOST}:${ES_PORT}
      LOGGER_PATH: "/app/app_data/logs"
    depends_on:
      - admin_db
      - elasticsearch
    command: bash -c "sh /start.sh"

  movies_api:
    container_name: movies_api
    build:
      dockerfile: ./etc/compose/movies_api/Dockerfile
      context: .
    environment:
      MOVIES_API_PORT: ${MOVIES_API_PORT}

  redis:
    image: redis:6
    environment:
      - PORT:${REDIS_PORT}
    ports:
      - ${REDIS_PORT}:${REDIS_PORT}
    healthcheck:
      test:
        [
          "CMD",
          "redis-cli",
          "--raw",
          "incr",
          "-p",
          "${REDIS_PORT}",
          "ping"
        ]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always
    command: /bin/bash -c "redis-server --port ${REDIS_PORT}"

  af_db:
    image: postgres:13-alpine
    environment:
      POSTGRES_USER: ${DB_AF_USER}
      POSTGRES_PASSWORD: ${DB_AF_PASSWORD}
      POSTGRES_DB: ${DB_AF_NAME}
      POSTGRES_PORT: ${DB_AF_PORT}
      PGDATA: "/var/lib/postgresql/data"
    healthcheck:
      test:
        [
          "CMD",
          "pg_isready",
          "-U",
          "${DB_AF_USER}",
          "-d",
          "${DB_AF_NAME}",
          "-h",
          "${DB_AF_HOST}",
          "-p",
          "${DB_AF_PORT}"
        ]
      interval: 5s
      retries: 10
    volumes:
      - af_db_vol:/var/lib/postgresql/data
    command: -p ${DB_AF_PORT}

  airflow-init:
    <<: *airflow-common
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

  airflow-webserver:
    <<: *airflow-common
    command: webserver -p ${AF_PORT}
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "--fail",
          "http://localhost:${AF_PORT}/airflow/health"
        ]
      interval: 5s
      timeout: 10s
      retries: 10
    restart: always
    environment:
      <<: *airflow-common-env

  airflow-scheduler:
    <<: *airflow-common
    command: airflow scheduler
    container_name: airflow_scheduler
    restart: always

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    restart: always

  nginx:
    container_name: my_nginx
    image: nginx:1.21.6
    restart: always
    volumes:
      - ./src/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./src/nginx/conf.d:/etc/nginx/conf.d
      - admin_static:/data/static
    environment:
      ADMIN_PORT: ${ADMIN_PORT}
      AF_PORT: ${AF_PORT}
      API_PORT: ${MOVIES_API_PORT}
      DOLLAR: "$"
      NGINX_PORT: ${NGINX_PORT}
    ports:
      - ${NGINX_PORT}:${NGINX_PORT}
    depends_on:
      - admin_panel
      - airflow-webserver
      - movies_api
    command: >
      /bin/bash -c "
        envsubst < /etc/nginx/conf.d/servers.conf.template > /etc/nginx/conf.d/servers.conf &&
        nginx -g 'daemon off;'
      "

  spark-master:
    build:
      context: .
      dockerfile: ./etc/compose/spark/Dockerfile
    hostname: spark-master
    ports:
      - ${SPARK_MASTER_WEBUI_PORT}:${SPARK_MASTER_WEBUI_PORT}
      - ${SPARK_MASTER_PORT}:${SPARK_MASTER_PORT}
    environment:
      - SPARK_LOCAL_IP=spark-master
      - SPARK_MASTER_PORT=${SPARK_MASTER_PORT}
      - SPARK_MASTER_WEBUI_PORT=${SPARK_MASTER_WEBUI_PORT}
    command: "/start-master.sh"

  spark-worker:
    build:
      context: .
      dockerfile: ./etc/compose/spark/Dockerfile
    depends_on:
      - spark-master
    ports:
      - ${SPARK_WORKER_WEBUI_PORT}
    environment:
      - SPARK_MASTER=spark://spark-master:${SPARK_MASTER_PORT}
      - SPARK_WORKER_WEBUI_PORT=${SPARK_WORKER_WEBUI_PORT}
    command: "/start-worker.sh"

volumes:
  admin_db_vol:
  af_db_vol:
  admin_static:
  af_logs:
  af_plugins:
  af_cfg:
